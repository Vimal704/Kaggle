{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9219215,"sourceType":"datasetVersion","datasetId":5575212},{"sourceId":99026,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":83076,"modelId":107368}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom torch.utils.data import Dataset, DataLoader\nimport zipfile\nimport pandas as pd\nimport os\nimport torch\nimport io\nimport numpy as np\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.landmark_frame = pd.read_csv(csv_file,names=['img_name', 'class'])\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmark_frame)\n\n        # Normalization of images as described in the paper\n    def normalize(self,image):\n        image[0,:,:] = (image[0,:,:]-torch.mean(image[0,:,:].float()))/torch.std(image[0,:,:].float())\n        image[1,:,:] = (image[1,:,:]-torch.mean(image[1,:,:].float()))/torch.std(image[1,:,:].float())\n        image[2,:,:] = (image[2,:,:]-torch.mean(image[2,:,:].float()))/torch.std(image[2,:,:].float())\n        return image\n        \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        classes = {'malignant':1.0, 'benign':0.0}\n        img_name = os.path.join(self.root_dir, self.landmark_frame.iloc[idx, 0]+'.jpg').replace('\\\\','/')\n        label = self.landmark_frame.iloc[idx, 1]\n        image = read_image(img_name)\n        \n        if self.transform:\n            sample = [image, classes[label]]\n            sample[0] = self.transform(sample[0])\n        else:\n            sample = [image,label]\n            transform2 = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((224,224)),\n                transforms.ToTensor()\n                ])\n            sample[0] = transform2(sample[0])\n\n        sample[0] = self.normalize(sample[0])\n        return sample\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224,224)),\n    transforms.RandomRotation(degrees=(-180, 180)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:51:32.105319Z","iopub.execute_input":"2024-08-22T06:51:32.105729Z","iopub.status.idle":"2024-08-22T06:51:32.111649Z","shell.execute_reply.started":"2024-08-22T06:51:32.105702Z","shell.execute_reply":"2024-08-22T06:51:32.110665Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"melanoma_dataset = MelanomaDataset('/kaggle/input/isbi2016-isic-part3/ISBI2016_ISIC_Part3_Training_GroundTruth.csv', '/kaggle/input/isbi2016-isic-part3/ISBI2016_ISIC_Part3_Training_Data/ISBI2016_ISIC_Part3_Training_Data',transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:51:33.147679Z","iopub.execute_input":"2024-08-22T06:51:33.148548Z","iopub.status.idle":"2024-08-22T06:51:33.158201Z","shell.execute_reply.started":"2024-08-22T06:51:33.148510Z","shell.execute_reply":"2024-08-22T06:51:33.157230Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\ndataloader = DataLoader(melanoma_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:51:34.700886Z","iopub.execute_input":"2024-08-22T06:51:34.701593Z","iopub.status.idle":"2024-08-22T06:51:34.706169Z","shell.execute_reply.started":"2024-08-22T06:51:34.701553Z","shell.execute_reply":"2024-08-22T06:51:34.705170Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = resnet50(weights = ResNet50_Weights.IMAGENET1K_V2)\n# model.fc = nn.Linear(2048,1, bias=False)\n# model = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/melanoma_resnet50/pytorch/default/1/MelanomaResNet50'\nmodel = resnet50()\nmodel.fc = nn.Linear(2048, 1, bias=False)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(PATH, map_location=device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the whole model on our dataset\nepochs = 100\nlearning_rate = 1e-5\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\nfor epoch in range(epochs):\n    for idx, (img, label) in enumerate(dataloader):\n        img = img.to(device)\n        label = torch.Tensor(label)\n        label = label.unsqueeze(1).to(device)\n\n        # forward pass\n        y_pred = model(img)\n        loss = criterion(y_pred, label)\n\n        # backward pass\n        loss.backward()\n\n        # weight update\n        optimizer.step()\n\n        # making gradint zero\n        optimizer.zero_grad()\n    print(epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/working/MelanomaResNet50_v2'\ntorch.save(model.state_dict(),PATH)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:53:22.651340Z","iopub.execute_input":"2024-08-22T06:53:22.651968Z","iopub.status.idle":"2024-08-22T06:53:22.655920Z","shell.execute_reply.started":"2024-08-22T06:53:22.651933Z","shell.execute_reply":"2024-08-22T06:53:22.654985Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model = resnet50()\nmodel.fc = nn.Linear(2048, 1, bias=False)\nmodel = model.to(device)\nmodel.load_state_dict(torch.load(PATH, map_location=device))","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:53:27.029735Z","iopub.execute_input":"2024-08-22T06:53:27.030124Z","iopub.status.idle":"2024-08-22T06:53:27.615648Z","shell.execute_reply.started":"2024-08-22T06:53:27.030095Z","shell.execute_reply":"2024-08-22T06:53:27.614722Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"melanoma_dataset_test = MelanomaDataset('/kaggle/input/isbi2016-isic-part3/ISBI2016_ISIC_Part3_Test_GroundTruth.csv', '/kaggle/input/isbi2016-isic-part3/ISBI2016_ISIC_Part3_Test_Data/ISBI2016_ISIC_Part3_Test_Data')\n# batch_size = 8\ndataloader_test = DataLoader(melanoma_dataset_test, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:53:28.794764Z","iopub.execute_input":"2024-08-22T06:53:28.795153Z","iopub.status.idle":"2024-08-22T06:53:28.803730Z","shell.execute_reply.started":"2024-08-22T06:53:28.795122Z","shell.execute_reply":"2024-08-22T06:53:28.802852Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Accuracy \ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for (image, label) in dataloader_test:\n        image = image.to(device)\n        label = torch.Tensor(label).unsqueeze(1).to(device)\n        \n        output = model(image)\n        prediction = []\n        for i in output:\n            if i>0:\n                prediction.append(1)\n            else:\n                prediction.append(0)\n        \n#         prediction = torch.max(output.data,1)\n        prediction = torch.Tensor(prediction).unsqueeze(1).to(device)\n        total += label.size(0)\n        correct +=  (prediction==label).sum().item()\n\nprint(f'The Accuracy of the model is {100*correct/total}')","metadata":{"execution":{"iopub.status.busy":"2024-08-22T06:53:52.591110Z","iopub.execute_input":"2024-08-22T06:53:52.591475Z","iopub.status.idle":"2024-08-22T06:54:24.178184Z","shell.execute_reply.started":"2024-08-22T06:53:52.591448Z","shell.execute_reply":"2024-08-22T06:54:24.177206Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"The Accuracy of the model is 92.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}